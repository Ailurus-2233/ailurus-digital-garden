---
{"dg-publish":true,"permalink":"/04//05//09/","title":"09-大数据架构","tags":["软考","系统架构设计师"]}
---


所属知识库：[[04-知识仓库/归纳目录/05-学习笔记/系统架构设计师学习笔记\|系统架构设计师学习笔记]]

## 1 概述

本章只会在**案例，论文**中着重考察。鉴于现在命题考察形式重点关注**图文表格**

案例部分：特别是大数据 Lamda 和 Kappa 的特点，架构图等已经在 2023 年案例中重点考察过。

大数据主题的论文也是每年常考的热点主题。2023 年考察过《论多数据源集成的应用与实现》，2022 年考察过《论湖仓一体架构及其应用》，2019 年考察过《论数据湖技术及其应用》，2013 年考察过《论分布式存储系统架构设计》，2010 年考察过《论数据挖掘技术的应用》，2024 年考察过《论大数据 Lambda 架构》。

**此处由于案例去年已经考到，今年出题角度一定会发生变化。**

**大数据处理系统分析**：
- 大数据处理系统面临三大挑战：处理非结构化和半结构化数据、描述大数据的复杂性和不确定性特征、数据异构性与决策异构性的关系对大数据知识发现与管理决策的影响。
- 大数据处理系统架构具有八大特征：鲁棒性和容错性、低延迟读取和更新能力、横向扩容、通用性、延展性、即席查询能力、最少维护能力、可调试性。

**Lambda 架构**：
- Lambda 架构设计目的在于提供一个能满足大数据系统关键特性的架构，包括高容错、低延迟、可扩展等。
- Lambda 架构可分解为三层：批处理层、加速层和服务层。
- 批处理层负责处理离线数据，使用分布式文件系统存储和处理数据。
- 加速层负责处理实时数据流，提供低延迟的查询结果。
- 服务层整合批处理层和加速层的结果，为用户提供统一的访问接口。

**Kappa 架构**：
- Kappa 架构在 Lambda 架构基础上进行了优化，删除了批处理层，使用消息队列替代。
- Kappa 架构的核心是流处理，数据在数据湖层面存储，需要离线分析时通过消息队列重播数据。
- Kappa 架构的优点是统一了实时和离线代码，维护方便，缺点是对实时计算系统的能力要求较高。

## 2 ※大数据处理系统分析

**大数据处理系统三大挑战**：
- 如何利用信息技术等手段处理非结构化和半结构化数据。
- 大数据复杂性、不确定性特征描述的刻画方法及大数据的系统建模。
- 数据异构性与决策异构性的关系对大数据知识发现与管理决策的影响。

**大数据处理系统架构八大特征**：
- **鲁棒性和容错性**：系统能够在出现故障时继续运行并保持稳定。
- **低延迟读取和更新能力**：系统能够快速响应数据的读取和更新请求。
- **横向扩容**：系统可以通过增加更多的机器资源来维持性能，实现线性扩展。
- **通用性**：系统能够处理多种类型的数据和任务。
- **延展性**：系统能够方便地添加新的功能和组件。
- **即席查询能力**：系统支持用户进行临时、即兴的查询。
- **最少维护能力**：系统需要较少的人工干预和维护。
- **可调试性**：系统具备良好的调试和故障排查能力。

## 3 ※Lambda 架构

Lambda 架构设计目的在于提供一个能满足大数据系统关键特性的架构，包括==高容错、低延迟、可扩展==等。其==整合离线计算与实时计算==，融合不可变性、读写分离和复杂性隔离等原则，==可集成 Hadoop、Kafka、Spark、Storm 等各类大数据组件==。Lambda 是用于==同时处理离线和实时数据的，可容错的，可扩展的分布式系统==。它具备==强鲁棒性，提供低延迟和持续更新==。可用于机器学习、物联网和流处理场景。Lambda 架构的诞生离不开很多现有设计思想和架构的铺垫，如事==件溯源架构和命令查询分离架构==。

**Lambda 架构介绍**：
- **设计目的**：提供一个能满足大数据系统关键特性的架构，包括高容错、低延迟、可扩展等。
- **架构分解**：分为三层，即批处理层、加速层和服务层。
- **整合离线计算与实时计算**：融合不可变性、读写分离和复杂性隔离等原则，可集成 Hadoop、Kafka、Spark、Storm 等各类大数据组件。

==Batch Layer 和 Speed Layer 的设计依据包括以下几点==：

- **容错性**：Speed Layer 中处理的数据也会不断写入 Batch Layer。当 Batch Layer 中重新计算的数据集包含 Speed Layer 处理的数据集后，当前的 Real-time View 可以丢弃。这意味着 Speed Layer 处理中引入的错误，在 Batch Layer 重新计算时都可以得到修正。这一点体现了 CAP 理论中的最终一致性（Eventual Consistency）。
- **复杂性隔离**：Batch Layer 处理的是离线数据，可以很好地掌控。Speed Layer 采用增量算法处理实时数据，复杂性比 Batch Layer 要高很多。通过分开 Batch Layer 和 Speed Layer，把复杂性隔离到 Speed Layer，可以很好地提高整个系统的鲁棒性和可靠性。
- **横向扩容**：当数据量/负载增大时，可扩展性的系统通过增加更多的机器资源来维持性能。也就是常说的系统需要线性可扩展，通常采用 scale out（通过增加机器的个数）而不是 scale up（通过增强机器的性能）。

**批处理层 (Batch Layer)**：
- **主要职责**：处理离线数据。
- **数据属性**：数据是原始的、不可变的、永远是真实的。
- **特点**：
	- Monoid 特性是一种数学概念，在分布式计算中有着重要的应用。具体来说，Monoid 特性指的是可以在数学上做分布和合并的操作。以下是 Monoid 特性的几个关键点：**分布和合并操作**：Monoid 特性允许将计算分解到多台机器上并行运算，然后再结合各自的部分运算结果得到最终结果。**部分运算结果的储存和共享**：部分运算结果可以储存下来被别的运算共享利用（如果该运算也包含相同的部分子运算），从而减少重复运算的工作量。**容错性和可扩展性**：利用 Monoid 特性，系统可以在处理故障和错误时，通过重新计算部分结果来恢复最终结果，提高了系统的容错性和可扩展性。在 Lambda 架构的 Batch Layer 中，Monoid 特性被用来优化数据处理过程，使得大规模数据的处理可以高效地在多台机器上并行执行。
		- 高可靠性：使用容错性较强的分布式文件系统（如 Hadoop HDFS）存储和处理数据。
		- 长时间窗口：可以使用较长的时间窗口进行数据处理，不要求实时性。
		- 复杂计算：进行复杂的数据计算和分析任务，如大规模数据聚合、数据清洗、机器学习模型训练等。

==如果预先在数据集上计算并保存查询函数的结果，查询的时候就可以直接返回结果（或通过简单的加工运算就可得到结果）而无需重新进行完整费时的计算了==。这里可以把 Batch Layer 看成是一个数据预处理的过程。我们把针对查询预先计算并保存的结果称为 View，View 是 Lamba 架构的一个核心概念，==它是针对查询的优化，通过 View 即可以快速得到查询结果==。

**加速层 (Speed Layer)**：
- **主要职责**：处理实时数据流，提供低延迟的查询结果。
- **特点**：
		- 实时性：处理实时数据流，快速响应查询请求。
		- 部分数据集：处理的数据集通常是部分数据，生成增量更新以保持数据的最新状态。
		- 简单计算：执行较简单的计算任务，如数据过滤、聚合、索引等。由于实时性要求较高，计算任务需要轻量级和高效。

| 对比       | Speed Layer                                | Batch Layer                                      |
| ---------- | ------------------------------------------ | ------------------------------------------------ |
| 处理的数据 | 处理的是最近的增量数据流                   | 处理的是全体数据集                               |
| 数据更新   | 接收到新数据时不断更新 Real-time View。    | 根据全体离线数据集直接得到 Batch View            |
| 实时性     | 提供低延迟的查询结果，实时响应查询请求     | 不要求实时性，可以使用较长的时间窗口进行数据处理 |
| 数据集     | 处理的数据集通常是部分数据，而不是全部数据 | 处理的数据集是全部数据                           |
| 计算任务 | 执行较简单的计算任务，如数据过滤、聚合、索引等 | 可以进行复杂的数据计算和分析任务，例如大规模数据聚合、数据清洗、机器学习模型训练等 |

**服务层 (Serving Layer)**：
- **主要职责**：整合批处理层和加速层的结果，提供统一的查询接口。
- **特点**：
		- 统一查询接口：将批处理层和加速层的结果进行整合，为用户提供统一的查询接口。
		- 数据合并：将批处理层和加速层的结果进行合并，保证查询结果的完整性和一致性。
		- 数据展示和分发：将查询结果展示给用户，并提供数据分发接口，将数据发送给其他系统或应用。

**Lambda 架构的实现**：
- **技术选型**：Hadoop (HDFS) 作为主数据存储层，Spark 或 Storm 构成速度层，提供快速的数据处理能力，HBase 或 Cassandra 作为服务层，提供实时的数据访问和更新，Hive 用于创建可查询的视图。

**Lambda 架构优缺点**：
- **优点**：容错性好、查询灵活度高、易伸缩、易扩展、全场景覆盖。
- **缺点**：编码开销大、针对具体场景重新离线训练一遍益处不大、重新部署和迁移成本很高。

## 4 ※Kappa 架构

**Kappa 架构介绍**
- **优化基础**：Kappa 架构在 Lambda 架构的基础上进行了优化，删除了 Batch Layer 的架构，将数据通道以消息队列进行替代。
- **数据处理方式**：依旧以流处理为主，数据在数据湖层面进行了存储，当需要进行离线分析或者再次计算的时候，则将数据湖的数据再次经过消息队列重播一次。

**Kappa 架构的实现**：
- **实时层**：负责实时数据处理。
- **服务层**：提供数据查询和服务接口。
- **数据存储**：在数据湖层面进行存储。

**优点**：
- **统一代码**：将实时和离线代码统一起来，方便维护且统一了数据口径的问题。

**缺点**：
- **性能瓶颈**：消息中间件缓存的数据量和回溯数据有性能瓶颈。
- **数据丢失风险**：在实时数据处理时，遇到大量不同的实时流进行关联时，非常依赖实时计算系统的能力，可能因为数据流先后顺序问题导致数据丢失。
- **稳定性问题**：在抛弃了离线数据处理模块的时候，同时也抛弃了离线计算更加稳定可靠的特点。

**Kappa+ 架构**：
- **核心思想**：让流计算框架直接读取 HDFS 中的数据仓库数据，实现实时计算和历史数据回溯计算，无需保存日志或复制数据到消息队列。
- **任务分类**：将任务分为无状态任务（并行扫描全量数据）和时间窗口任务（基于时间分区对数据进行计算）。

**数据分析架构**：
- **架构组合**：在基于使用 Kafka+Flink 构建 Kappa 流计算数据架构的基础上，==再利用 Kafka 对接组合 Elastic-Search 实时分析引擎，部分弥补其数据分析能力==。

## 5 ※Lambda 架构和 Kappa 架构对比

| 对比内容               | Lambda 架构                                                            | Kappa 架构                                                           |
| ---------------------- | ---------------------------------------------------------------------- | -------------------------------------------------------------------- |
| 复杂度与开发、维护成本 | 需要维护两套系统（引擎），复杂度较高，维护成本也高                     | 只需要维护一套系统（引擎），复杂度低，维护成本低                     |
| 计算开销               | 需要一直运行批处理和实时计算，计算开销大                               | 必要时进行全量计算，计算开销相对较小                                 |
| 实时性                 | 满足实时性需求                                                         | 满足实时性需求，流式全量处理，吞吐量相对较低，数据处理能力相对较弱   |
| 历史数据处理能力       | 批式全量处理，吞吐量大，历史数据处理能力强                             | 通过消息队列重播数据湖的数据进行离线分析或再次计算                   |
| 适用场景               | 适用于需要同时处理离线和实时数据场景，容错性好，但复杂度高，计算开销大 | 适用于实时数据处理场景，维护简单，计算开销小，但历史数据处理能力较弱 |

## 6 例图汇总

### 6.1 基于 Lambda 架构的某网奥运系统

![IMG-302EFF4948053B24BD1674A6CE0D2829.png](/img/user/00-%E7%B3%BB%E7%BB%9F%E6%96%87%E4%BB%B6/%E6%96%87%E6%A1%A3%E9%99%84%E4%BB%B6/04-%E7%9F%A5%E8%AF%86%E4%BB%93%E5%BA%93/%E7%9F%A5%E8%AF%86%E5%8D%95%E5%85%83/05-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%B8%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/09-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84-%E9%99%84%E4%BB%B6/IMG-302EFF4948053B24BD1674A6CE0D2829.png)

数据采集层：

- **PC 端**：从个人电脑端收集数据。
- **App 端**：从移动应用端收集数据。
- **TV 端**：从电视端收集数据。

数据集成层：

- **实时数据集成**：通过 Flume 和 Kafka 实现实时数据的集成。
- **离线数据集成**：通过 ETL 和 Sqoop 实现离线数据的集成。

数据存储层：

- **HDFS**：用于存储大规模数据集。
- **HBase**：用于实时数据访问和更新。
- **MemSQL**：用于存储和查询实时数据。

数据计算层：

- **离线计算**：使用 Hive、Impala、Hive、M-R 进行批量计算。
- **实时计算**：使用 Spark Streaming 进行实时增量计算。
- **合并计算**：使用 Spark 进行实时和离线数据的合并计算。

数据展现层：

- **当日概览**：展示当日的统计数据。
- **赛事回顾**：展示赛事相关的统计数据。

系统特点：

- **实时数据**：通过 Kafka 队列分发给 Spark Streaming 进行实时增量计算。
- **离线数据**：持续追加存储在 HDFS，由 Spark/MR 进行批量计算。
- **数据安全**：使用 Cloud 存储技术，确保数据安全可用性。
- **数据视图**：实时/离线计算结果分别更新到 Real-time view 和 Batch view。
- **合并计算**：将两个 view 合并为最终结果，由 MemSQL/HBase 存储。

### 6.2 基于 Lambda 架构的某网广告平台

![IMG-D074D13A8B4FF39D3046081786E1C35D.png](/img/user/00-%E7%B3%BB%E7%BB%9F%E6%96%87%E4%BB%B6/%E6%96%87%E6%A1%A3%E9%99%84%E4%BB%B6/04-%E7%9F%A5%E8%AF%86%E4%BB%93%E5%BA%93/%E7%9F%A5%E8%AF%86%E5%8D%95%E5%85%83/05-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%B8%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/09-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84-%E9%99%84%E4%BB%B6/IMG-D074D13A8B4FF39D3046081786E1C35D.png)

数据源：

- **下单信息**：用户下单的相关数据。
- **浏览、付款信息**：用户浏览和付款的相关数据。

数据集成层：

- **Kafka**：用于实时数据传输和消息队列。

批处理层 (Batch Layer)：

- **HDFS**：用于存储大规模数据集。
- **Hive**：用于创建可查询的视图，使得数据可以被方便地查询和分析。
- **Batch Views**：预先计算并保存查询函数的结果，以便快速得到查询结果。

加速层 (Speed Layer)：

- **Spark Streaming**：用于实时数据处理，提供低延迟的查询结果。
- **Real-time Views**：根据实时数据流生成增量更新，保持数据的最新状态。

服务层 (Serving Layer)：

- **MySQL**：用于存储实时和批量处理后的数据。
- **Redis**：用于缓存实时数据，提高数据访问速度。
- **Java Query**：提供统一的查询接口，整合批处理层和加速层的结果。

系统特点

- **第一版**：采用典型的 Lambda 架构，批处理层和实时处理层分别计算离线数据和实时数据，服务层负责合并两部分数据。但这种架构服务层代码复杂，性能受限。
- **第二版**：改进了实时处理层，通过动态调整第三方 API 调用频率，获取更及时的曝光数据。批处理层也优化了离线数据的生成和导入 MySQL 的过程。
- **第三版**：进一步优化，在数据层面只维护一张包含所有指标的 MySQL 表，实时数据的更新也直接在业务服务中完成，大幅简化了服务层的逻辑。

### 6.3 基于 Kappa 架构的证券大数据系统

![IMG-3623C908E518BCD94E990D92B68D6A87.png](/img/user/00-%E7%B3%BB%E7%BB%9F%E6%96%87%E4%BB%B6/%E6%96%87%E6%A1%A3%E9%99%84%E4%BB%B6/04-%E7%9F%A5%E8%AF%86%E4%BB%93%E5%BA%93/%E7%9F%A5%E8%AF%86%E5%8D%95%E5%85%83/05-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%B8%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/09-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84-%E9%99%84%E4%BB%B6/IMG-3623C908E518BCD94E990D92B68D6A87.png)

该平台采用 Kappa 架构，使用 Flink 进行实时数据处理，Elasticsearch 和 OpenTSDB 分别存储日志数据和时序指标数据。前端提供了丰富的搜索、分析和监控功能。为降低开发难度，该平台支持基于 Ruby 脚本的可配置日志解析，并将其转换为 Flink 可执行任务，充分利用了大数据集群的处理能力。

### 6.4 基于 Kappa 架构的电商智能决策大数据系统

![IMG-F542FA03B94FC61FD5A139BEDFED2455.png](/img/user/00-%E7%B3%BB%E7%BB%9F%E6%96%87%E4%BB%B6/%E6%96%87%E6%A1%A3%E9%99%84%E4%BB%B6/04-%E7%9F%A5%E8%AF%86%E4%BB%93%E5%BA%93/%E7%9F%A5%E8%AF%86%E5%8D%95%E5%85%83/05-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%B8%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/09-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84-%E9%99%84%E4%BB%B6/IMG-F542FA03B94FC61FD5A139BEDFED2455.png)

该系统基于 Kappa 架构，使用 Flink 统一实时处理流数据，存储于 Hive 和 Tair，提高了计算结果准确性。决策服务采用微服务架构，客户端本地存储参数，提高了业务响应及时性。实时处理用户点击、下单、广告曝光出价等数据，使用 Flink 计算集群过滤、聚合数据，存入 Tair 分布式缓存。决策服务端从 Tair 读取数据，利用多种算法框架，计算生成决策参数和模型，存储于 Tair 和 Hive。决策服务客户端部署在业务系统中，通过 Zookeeper 获取服务端最新参数，本地存储并提供接口供业务调用。
